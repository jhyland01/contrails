{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "data_path = Path('/../train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = {\n",
    "    \"data_path\": \"../\",\n",
    "    \"model\": {\n",
    "        \"encoder_name\": \"timm-resnest26d\",\n",
    "        \"loss_smooth\": 1.0,\n",
    "        \"optimizer_params\": {\"lr\": 0.003, \"weight_decay\": 0.0},\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"CosineAnnealingLR\",\n",
    "            \"params\": {\n",
    "                \"CosineAnnealingLR\": {\"T_max\": 500, \"eta_min\": 1e-06, \"last_epoch\": -1},\n",
    "                \"ReduceLROnPlateau\": {\n",
    "                    \"factor\": 0.31622776601,\n",
    "                    \"mode\": \"min\",\n",
    "                    \"patience\": 3,\n",
    "                    \"verbose\": True,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"seg_model\": \"Unet\",\n",
    "    },\n",
    "    \"output_dir\": \"models\",\n",
    "    \"progress_bar_refresh_rate\": 50,\n",
    "    \"seed\": 42,\n",
    "    \"train_bs\": 128,\n",
    "    \"use_aug\": True,\n",
    "    \"trainer\": {\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"max_epochs\": 30,\n",
    "        \"min_epochs\": 40,\n",
    "        \"accelerator\": \"mps\",\n",
    "        \"devices\": 1,\n",
    "    },\n",
    "    \"valid_bs\": 128,\n",
    "    \"workers\": 0,\n",
    "    \"device\": \"mps\",\n",
    "    \"folds\": {\n",
    "        \"n_splits\": 4,\n",
    "        \"random_state\": 42,\n",
    "        \"train_folds\": [0, 1, 2, 3]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ContrailsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_size=256, train=True, use_augmentations=False):\n",
    "\n",
    "        self.df = df\n",
    "        self.trn = train\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.image_size = image_size\n",
    "        self.use_augmentations = use_augmentations\n",
    "        if image_size != 256:\n",
    "            self.resize_image = T.transforms.Resize(image_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        con_path = row.path\n",
    "        con = np.load(str(con_path))\n",
    "\n",
    "        img = con[..., :-1]\n",
    "        label = con[..., -1]\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        if self.image_size != 256:\n",
    "            img = self.resize_image(img)\n",
    "\n",
    "        if self.use_augmentations:\n",
    "            sample = {\"image\" : img.swapaxes(0, 2).swapaxes(0, 1), \"mask\": msk.swapaxes(0, 2).swapaxes(0, 1)}\n",
    "            transformed_sample = self.use_augmentations(**sample)\n",
    "            img, msk = transformed_sample[\"image\"].swapaxes(0, 2).swapaxes(1, 2).copy(), transformed_sample[\"mask\"].swapaxes(0, 2).swapaxes(1, 2).copy()\n",
    "\n",
    "        img = self.normalize_image(img)\n",
    "\n",
    "        return img.float(), label.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning module\n",
    "\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.callbacks import ProgressBar\n",
    "from torchmetrics.functional import dice, f1_score, jaccard_index\n",
    "# from torchmetrics import IoU\n",
    "\n",
    "bar = ProgressBar()\n",
    "\n",
    "seg_models = {\n",
    "    \"Unet\": smp.Unet,\n",
    "    \"Unet++\": smp.UnetPlusPlus,\n",
    "    \"MAnet\": smp.MAnet,\n",
    "    \"Linknet\": smp.Linknet,\n",
    "    \"FPN\": smp.FPN,\n",
    "    \"PSPNet\": smp.PSPNet,\n",
    "    \"PAN\": smp.PAN,\n",
    "    \"DeepLabV3\": smp.DeepLabV3,\n",
    "    \"DeepLabV3+\": smp.DeepLabV3Plus,\n",
    "}\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = torch.sigmoid(logits)\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (logits * targets).sum()\n",
    "        return 1 - (2.0 * intersection + self.smooth) / (\n",
    "            logits.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model = seg_models[config[\"seg_model\"]](\n",
    "            encoder_name=config[\"encoder_name\"],\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        ).to(self.device)\n",
    "        self.loss_module = DiceLoss(smooth=config[\"loss_smooth\"])\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        imgs = batch.to(self.device)\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), **self.config[\"optimizer_params\"])\n",
    "\n",
    "        if self.config[\"scheduler\"][\"name\"] == \"CosineAnnealingLR\":\n",
    "            scheduler = CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n",
    "        elif self.config[\"scheduler\"][\"name\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        labels = labels.unsqueeze(1)\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=16)\n",
    "\n",
    "        for param_group in self.trainer.optimizers[0].param_groups:\n",
    "            lr = param_group[\"lr\"]\n",
    "        self.log(\"lr\", lr, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        labels = labels.unsqueeze(1)\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.val_step_outputs)\n",
    "        all_labels = torch.cat(self.val_step_labels)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        # val_dice = dice(all_preds, all_labels.long())\n",
    "        # val_f1 = f1_score(all_preds.sigmoid(), all_labels.long(), task = \"binary\")\n",
    "        val_iou = jaccard_index(num_classes=2, task='binary', preds=all_preds.sigmoid(), target=all_labels.long())\n",
    "        self.log(\"val_iou\", val_iou, on_step=False, on_epoch=True, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m train_df \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39miloc[train_index]\n\u001b[1;32m     27\u001b[0m valid_df \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39miloc[valid_index]\n\u001b[0;32m---> 29\u001b[0m dataset_train \u001b[39m=\u001b[39m ContrailsDataset(train_df, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     30\u001b[0m dataset_validation \u001b[39m=\u001b[39m ContrailsDataset(valid_df, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m data_loader_train \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     33\u001b[0m     dataset_train, batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mtrain_bs\u001b[39m\u001b[39m\"\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mworkers\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m )\n",
      "Cell \u001b[0;32mIn[81], line 11\u001b[0m, in \u001b[0;36mContrailsDataset.__init__\u001b[0;34m(self, df, image_size, train, fold, n_splits)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, df, image_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fold\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m     10\u001b[0m     skf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39mn_splits)\n\u001b[0;32m---> 11\u001b[0m     train_indices, val_indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(skf\u001b[39m.\u001b[39msplit(df, df[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m]))[fold]  \u001b[39m# assuming df['labels'] exists and is your target\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[train_indices] \u001b[39mif\u001b[39;00m train \u001b[39melse\u001b[39;00m df\u001b[39m.\u001b[39miloc[val_indices]\n\u001b[1;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrn \u001b[39m=\u001b[39m train\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "from pprint import pprint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "contrails = os.path.join(config[\"data_path\"], \"contrails/\")\n",
    "train_path = os.path.join(config[\"data_path\"], \"train_df.csv\")\n",
    "valid_path = os.path.join(config[\"data_path\"], \"valid_df.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "valid_df = pd.read_csv(valid_path)\n",
    "\n",
    "train_df[\"path\"] = contrails + train_df[\"record_id\"].astype(str) + \".npy\"\n",
    "valid_df[\"path\"] = contrails + valid_df[\"record_id\"].astype(str) + \".npy\"\n",
    "\n",
    "dataset_train = ContrailsDataset(train_df, train=True, use_augmentations=config[\"use_aug\"])\n",
    "dataset_validation = ContrailsDataset(valid_df, train=False)\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train, batch_size=config[\"train_bs\"], shuffle=True, num_workers=config[\"workers\"]\n",
    ")\n",
    "data_loader_validation = DataLoader(\n",
    "    dataset_validation, batch_size=config[\"valid_bs\"], shuffle=False, num_workers=config[\"workers\"]\n",
    ")\n",
    "\n",
    "pl.seed_everything(config[\"seed\"])\n",
    "\n",
    "filename = f\"model\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_iou\",\n",
    "    dirpath=config[\"output_dir\"],\n",
    "    mode=\"max\",\n",
    "    filename=filename,\n",
    "    save_top_k=1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "progress_bar_callback = TQDMProgressBar(refresh_rate=config[\"progress_bar_refresh_rate\"])\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=1)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, progress_bar_callback], logger=None, **config[\"trainer\"]\n",
    ")\n",
    "\n",
    "model = LightningModule(config[\"model\"])\n",
    "\n",
    "trainer.fit(model, data_loader_train, data_loader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'models/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([train_df, valid_df])\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.to_csv(\"../data_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
