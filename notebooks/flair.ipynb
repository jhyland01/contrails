{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "# data_path = Path('/../train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = '/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/flair/FLAIR-1-AI-Challenge/flair_aerial_train'\n",
    "# mask_folder = '/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/flair/FLAIR-1-AI-Challenge/flair_labels_train'\n",
    "\n",
    "# def get_data_paths (path, filter):\n",
    "#     for path in Path(path).rglob(filter):\n",
    "#          yield path.resolve().as_posix()\n",
    "\n",
    "# images = sorted(list(get_data_paths(Path(train_folder), 'IMG*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n",
    "# masks  = sorted(list(get_data_paths(Path(mask_folder), 'MSK*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/flair/FLAIR-1-AI-Challenge/imgs'\n",
    "mask_folder = '/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/flair/FLAIR-1-AI-Challenge/msks'\n",
    "\n",
    "def get_data_paths (path, filter):\n",
    "    for path in Path(path).rglob(filter):\n",
    "         yield path.resolve().as_posix()\n",
    "\n",
    "images = sorted(list(get_data_paths(Path(train_folder), '*.npy')))\n",
    "masks  = sorted(list(get_data_paths(Path(mask_folder), '*.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def Normalise(arr_band):\n",
    "    \n",
    "    return StandardScaler().fit_transform(arr_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "\n",
    "# def read_img(raster_file: str) -> np.ndarray:\n",
    "#     with rasterio.open(raster_file) as src:\n",
    "#         arr_1, arr_2, arr_3, arr_4, arr_5 = Normalise(src.read(1)), Normalise(src.read(2)), Normalise(src.read(3)), Normalise(src.read(4)), Normalise(src.read(5))\n",
    "#         return sum((arr_1, arr_2, arr_3, arr_4, arr_5))\n",
    "\n",
    "# def read_msk(raster_file: str) -> np.ndarray:\n",
    "#     with rasterio.open(raster_file) as src_msk:\n",
    "#         array = src_msk.read()[0]\n",
    "#         array[array > 13] = 13\n",
    "#         array = array-1\n",
    "#         array = np.stack([array == i for i in range(13)], axis=0)\n",
    "#         return array\n",
    "    \n",
    "# for i in range(len(images)):\n",
    "#     img = read_img(images[i]).astype(np.float32)\n",
    "#     # msk = read_msk(masks[i])\n",
    "#     # save the formatted images\n",
    "#     np.save(f'/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/flair/FLAIR-1-AI-Challenge/imgs/img_{i}.npy', img)\n",
    "#     # np.save(f'/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/flair/FLAIR-1-AI-Challenge/msks/msk_{i}.npy', msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# resnet34, efficientnet-b1, resnext101_32x8d_wsl, resnext101_32x16d_wsl\n",
    "config = {\n",
    "    \"data_path\": \"../\",\n",
    "    \"model\": {\n",
    "        \"encoder_name\": \"timm-resnest26d\",\n",
    "        \"loss_smooth\": 1.0,\n",
    "        \"optimizer_params\": {\"lr\": 0.003, \"weight_decay\": 0.0},\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"CosineAnnealingLR\",\n",
    "            \"params\": {\n",
    "                \"CosineAnnealingLR\": {\"T_max\": 500, \"eta_min\": 1e-06, \"last_epoch\": -1},\n",
    "                \"ReduceLROnPlateau\": {\n",
    "                    \"factor\": 0.3162,\n",
    "                    \"mode\": \"min\",\n",
    "                    \"patience\": 4,\n",
    "                    \"verbose\": True,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        \"seg_model\": \"Unet\",\n",
    "    },\n",
    "    \"output_dir\": \"models\",\n",
    "    \"progress_bar_refresh_rate\": 10,\n",
    "    \"seed\": 42,\n",
    "    \"train_bs\":15,\n",
    "    \"trainer\": {\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"max_epochs\": 30,\n",
    "        \"min_epochs\": 20,\n",
    "        \"accelerator\": \"mps\",\n",
    "        \"devices\": 1,\n",
    "    },\n",
    "    \"valid_bs\": 15,\n",
    "    \"workers\": 0,\n",
    "    \"device\": \"mps\",\n",
    "    \"folds\": {\n",
    "        \"n_splits\": 4,\n",
    "        \"random_state\": 42,\n",
    "        \"train_folds\": [0, 1, 2, 3]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from skimage import img_as_float\n",
    "\n",
    "class FlairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, image_size=256, num_classes=13):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        # if image_size != 256:\n",
    "        #     self.resize_image = T.Resize((image_size, image_size))\n",
    "        #     self.resize_mask = T.Resize((image_size, image_size), interpolation=Image.NEAREST)\n",
    "            \n",
    "\n",
    "    def read_img(self, raster_file: str) -> np.ndarray:\n",
    "        with rasterio.open(raster_file) as src:\n",
    "            arr_1, arr_2, arr_3, arr_4, arr_5 = Normalise(src.read(1)), Normalise(src.read(2)), Normalise(src.read(3)), Normalise(src.read(4)), Normalise(src.read(5))\n",
    "            return sum((arr_1, arr_2, arr_3, arr_4, arr_5))\n",
    "\n",
    "    def read_msk(self, raster_file: str) -> np.ndarray:\n",
    "        with rasterio.open(raster_file) as src_msk:\n",
    "            array = src_msk.read()[0]\n",
    "            array[array > self.num_classes] = self.num_classes\n",
    "            array = array-1\n",
    "            array = np.stack([array == i for i in range(self.num_classes)], axis=0)\n",
    "            return array\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        mask_path = self.mask_paths[index]\n",
    "\n",
    "        img = np.load(image_path)\n",
    "        msk = np.load(mask_path)         \n",
    "        \n",
    "        # img = img_as_float(img)\n",
    "\n",
    "        # # Resize if necessary\n",
    "        # if self.image_size != 256:\n",
    "        #     img = self.resize_image(img)\n",
    "        #     label = self.resize_mask(label)\n",
    "\n",
    "        # Transform the image and mask to tensors\n",
    "        img = torch.as_tensor(img, dtype=torch.float).unsqueeze(0)\n",
    "        msk = torch.as_tensor(msk, dtype=torch.float)\n",
    "\n",
    "        # Normalize the image\n",
    "        # img = self.normalize_image(img)\n",
    "\n",
    "        return img, msk\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "# class ContrailsDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, df, image_size=256, train=True):\n",
    "\n",
    "#         self.df = df\n",
    "#         self.trn = train\n",
    "#         self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#         self.image_size = image_size\n",
    "#         if image_size != 256:\n",
    "#             self.resize_image = T.transforms.Resize(image_size)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         row = self.df.iloc[index]\n",
    "#         con_path = row.path\n",
    "#         con = np.load(str(con_path))\n",
    "\n",
    "#         img = con[..., :-1]\n",
    "#         label = con[..., -1]\n",
    "\n",
    "#         label = torch.tensor(label)\n",
    "\n",
    "#         img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "\n",
    "#         if self.image_size != 256:\n",
    "#             img = self.resize_image(img)\n",
    "\n",
    "#         img = self.normalize_image(img)\n",
    "\n",
    "#         return img.float(), label.float()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.callbacks import ProgressBar\n",
    "from torchmetrics.functional import dice, f1_score, jaccard_index\n",
    "import gc\n",
    "\n",
    "bar = ProgressBar()\n",
    "\n",
    "seg_models = {\n",
    "    \"Unet\": smp.Unet,\n",
    "    \"Unet++\": smp.UnetPlusPlus,\n",
    "    \"MAnet\": smp.MAnet,\n",
    "    \"Linknet\": smp.Linknet,\n",
    "    \"FPN\": smp.FPN,\n",
    "    \"PSPNet\": smp.PSPNet,\n",
    "    \"PAN\": smp.PAN,\n",
    "    \"DeepLabV3\": smp.DeepLabV3,\n",
    "    \"DeepLabV3+\": smp.DeepLabV3Plus,\n",
    "}\n",
    "\n",
    "# class DiceLoss(nn.Module):\n",
    "#     def __init__(self, smooth=1):\n",
    "#         super(DiceLoss, self).__init__()\n",
    "#         self.smooth = smooth\n",
    "\n",
    "#     def forward(self, logits, targets):\n",
    "#         logits = torch.sigmoid(logits)\n",
    "\n",
    "#         batch_size, num_classes, height, width = logits.size()\n",
    "\n",
    "#         logits = logits.reshape(batch_size, -1)\n",
    "#         targets = targets.reshape(batch_size, -1)\n",
    "\n",
    "#         intersection = (logits * targets).sum(dim=1)\n",
    "#         union = logits.sum(dim=1) + targets.sum(dim=1)\n",
    "#         dice_coeff = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "#         return 1 - dice_coeff.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = torch.sigmoid(logits)\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (logits * targets).sum()\n",
    "        return 1 - (2.0 * intersection + self.smooth) / (\n",
    "            logits.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = seg_models[config[\"seg_model\"]](\n",
    "            encoder_name=config[\"encoder_name\"],\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=1,\n",
    "            classes=13,\n",
    "            activation=None,\n",
    "        )\n",
    "        self.loss_module = DiceLoss(smooth=config[\"loss_smooth\"])\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        imgs = batch.to(self.device)\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), **self.config[\"optimizer_params\"])\n",
    "\n",
    "        if self.config[\"scheduler\"][\"name\"] == \"CosineAnnealingLR\":\n",
    "            scheduler = CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n",
    "        elif self.config[\"scheduler\"][\"name\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        # labels = labels.unsqueeze(1)\n",
    "        # labels = labels.expand(-1, 13, -1, -1)\n",
    "        preds = self.model(imgs)\n",
    "        del imgs\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=16)\n",
    "\n",
    "        for param_group in self.trainer.optimizers[0].param_groups:\n",
    "            lr = param_group[\"lr\"]\n",
    "        self.log(\"lr\", lr, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        # labels = labels.unsqueeze(1)\n",
    "        # labels = labels.expand(-1, 13, -1, -1)\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"val_loss\", loss , on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.val_step_outputs.append(preds)\n",
    "        # self.val_step_labels.append(labels)\n",
    "\n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     all_preds = torch.cat(self.val_step_outputs)\n",
    "    #     all_labels = torch.cat(self.val_step_labels)\n",
    "    #     self.val_step_outputs.clear()\n",
    "    #     self.val_step_labels.clear()\n",
    "    #     val_iou = jaccard_index(num_classes=13, preds=all_preds.sigmoid(), target=all_labels.long(), task='multiclass')\n",
    "    #     self.log(\"val_iou\", val_iou, on_step=False, on_epoch=True, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type     | Params\n",
      "-----------------------------------------\n",
      "0 | model       | Unet     | 24.0 M\n",
      "1 | loss_module | DiceLoss | 0     \n",
      "-----------------------------------------\n",
      "24.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.0 M    Total params\n",
      "96.139    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bc6a0c66c449ab9efe3c7fc66075ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd86ee3512b4ad7b22810b51ec34650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c0cf18ac03471e91de89112b4d9b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.679\n",
      "Epoch 0, global step 3292: 'val_loss' reached 0.67940 (best 0.67940), saving model to '/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/contrails/notebooks/models/Unet_timm-resnest26d-v3.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400c29eb000941c3a3fbf9fd905b4246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.671\n",
      "Epoch 1, global step 6584: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f749b40124d169362072d0d66a629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.629\n",
      "Epoch 2, global step 9876: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fc2d56bf45455c9771c40da7b12da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.602\n",
      "Epoch 3, global step 13168: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cecbc278093408ba9cc02964a27f60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 16460: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2725653f1aaf419fbdee758d8c034868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.597\n",
      "Epoch 5, global step 19752: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd371e49c58147e6be9a4d96fad3a258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.572\n",
      "Epoch 6, global step 23044: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25c037fa5554e888e632d6e0a71a3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 26336: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29737b47bc14f2583b564d0e818d4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 29628: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf77691c12e44eddafe58b5c61e41efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 32920: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886bd9ea3c1f4470981e8c9f9a5e3e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 36212: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af72a93988f640c1bebee0c9a01dfcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.572. Signaling Trainer to stop.\n",
      "Epoch 11, global step 39504: 'val_loss' was not in top 1\n",
      "Trainer was signaled to stop but the required `min_epochs=20` or `min_steps=None` has not been met. Training will continue...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35c7464076f47aeb24bafb3988b74f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.572. Signaling Trainer to stop.\n",
      "Epoch 12, global step 42796: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b5a7932a58427d898b007d911457d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.568\n",
      "Epoch 13, global step 46088: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489658c899a5451dbfac8b2375dd1c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 49380: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b259b6cbb3874a9399d9c5bc2ccc54a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.551\n",
      "Epoch 15, global step 52672: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7046cd0d6b41da942572217ffb8419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 55964: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154943cec362465aa367345c7acf7e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.524\n",
      "Epoch 17, global step 59256: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af38856a73764a01979ac476a53c3174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 62548: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109bc1f9980248388445453e6e322113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.518\n",
      "Epoch 19, global step 65840: 'val_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "from pprint import pprint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Split your data into train and validation sets, here I simply split it 80-20\n",
    "\n",
    "fraction = 1\n",
    "\n",
    "split_point = int(len(images) * 0.8 * fraction)\n",
    "train_split = int(len(images) * 0.8 * 0.8 * fraction)\n",
    "\n",
    "train_image_paths = images[:split_point]\n",
    "train_mask_paths = masks[:split_point]\n",
    "\n",
    "valid_image_paths = images[split_point:]\n",
    "valid_mask_paths = masks[split_point:]\n",
    "\n",
    "# Use your dataset class\n",
    "dataset_train = FlairDataset(train_image_paths, train_mask_paths)\n",
    "dataset_validation = FlairDataset(valid_image_paths, valid_mask_paths)\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train, batch_size=config[\"train_bs\"], shuffle=False, num_workers=config[\"workers\"]\n",
    ")\n",
    "data_loader_validation = DataLoader(\n",
    "    dataset_validation, batch_size=config[\"valid_bs\"], shuffle=False, num_workers=config[\"workers\"]\n",
    ")\n",
    "\n",
    "pl.seed_everything(config[\"seed\"])\n",
    "\n",
    "filename = f\"{config['model']['seg_model']}_{config['model']['encoder_name']}\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=config[\"output_dir\"],\n",
    "    mode=\"min\",\n",
    "    filename=filename,\n",
    "    save_top_k=1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "progress_bar_callback = TQDMProgressBar(refresh_rate=config[\"progress_bar_refresh_rate\"])\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=1)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, progress_bar_callback], logger=None, **config[\"trainer\"]\n",
    ")\n",
    "\n",
    "model = LightningModule(config[\"model\"])\n",
    "\n",
    "trainer.fit(model, data_loader_train, data_loader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the current model checkpoint\n",
    "trainer.save_checkpoint(os.path.join(config[\"output_dir\"], f\"{filename}.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30473), started 0:00:05 ago. (Use '!kill 30473' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), f'models/{filename}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x426ec4210>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training and validation loss\n",
    "plt.plot(trainer.callback_metrics['train_loss'])\n",
    "plt.plot(trainer.callback_metrics['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
